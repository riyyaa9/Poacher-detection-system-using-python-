{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-jXI3yn7zA7",
        "outputId": "4880aa76-3bd6-4d8d-aa4d-ff5e4c417508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 491 images belonging to 2 classes.\n",
            "Found 116 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "15/15 [==============================] - 129s 8s/step - loss: 0.7341 - accuracy: 0.7603 - val_loss: 0.1875 - val_accuracy: 0.9271\n",
            "Epoch 2/30\n",
            "15/15 [==============================] - 32s 2s/step - loss: 0.1878 - accuracy: 0.9259 - val_loss: 0.2046 - val_accuracy: 0.8958\n",
            "Epoch 3/30\n",
            "15/15 [==============================] - 33s 2s/step - loss: 0.1582 - accuracy: 0.9354 - val_loss: 0.2372 - val_accuracy: 0.9062\n",
            "Epoch 4/30\n",
            "15/15 [==============================] - 36s 2s/step - loss: 0.1189 - accuracy: 0.9521 - val_loss: 0.2334 - val_accuracy: 0.9167\n",
            "Epoch 5/30\n",
            "15/15 [==============================] - 34s 2s/step - loss: 0.0939 - accuracy: 0.9586 - val_loss: 0.1348 - val_accuracy: 0.9271\n",
            "Epoch 6/30\n",
            "15/15 [==============================] - 30s 2s/step - loss: 0.0709 - accuracy: 0.9760 - val_loss: 0.1334 - val_accuracy: 0.9375\n",
            "Epoch 7/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.0515 - accuracy: 0.9826 - val_loss: 0.1774 - val_accuracy: 0.9271\n",
            "Epoch 8/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.0597 - accuracy: 0.9688 - val_loss: 0.1696 - val_accuracy: 0.9479\n",
            "Epoch 9/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.0587 - accuracy: 0.9695 - val_loss: 0.1945 - val_accuracy: 0.9271\n",
            "Epoch 10/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.0530 - accuracy: 0.9804 - val_loss: 0.1492 - val_accuracy: 0.9167\n",
            "Epoch 11/30\n",
            "15/15 [==============================] - 30s 2s/step - loss: 0.0645 - accuracy: 0.9717 - val_loss: 0.3169 - val_accuracy: 0.8958\n",
            "Epoch 12/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.1108 - accuracy: 0.9586 - val_loss: 0.1328 - val_accuracy: 0.9688\n",
            "Epoch 13/30\n",
            "15/15 [==============================] - 30s 2s/step - loss: 0.0576 - accuracy: 0.9804 - val_loss: 0.1413 - val_accuracy: 0.9479\n",
            "Epoch 14/30\n",
            "15/15 [==============================] - 30s 2s/step - loss: 0.0380 - accuracy: 0.9869 - val_loss: 0.1958 - val_accuracy: 0.9167\n",
            "Epoch 15/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.0474 - accuracy: 0.9804 - val_loss: 0.1465 - val_accuracy: 0.9479\n",
            "Epoch 16/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.0377 - accuracy: 0.9847 - val_loss: 0.1974 - val_accuracy: 0.9167\n",
            "Epoch 17/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.0473 - accuracy: 0.9782 - val_loss: 0.1255 - val_accuracy: 0.9792\n",
            "Epoch 18/30\n",
            "15/15 [==============================] - 34s 2s/step - loss: 0.0602 - accuracy: 0.9717 - val_loss: 0.1666 - val_accuracy: 0.9375\n",
            "Epoch 19/30\n",
            "15/15 [==============================] - 30s 2s/step - loss: 0.1078 - accuracy: 0.9608 - val_loss: 0.1052 - val_accuracy: 0.9688\n",
            "Epoch 20/30\n",
            "15/15 [==============================] - 30s 2s/step - loss: 0.0606 - accuracy: 0.9826 - val_loss: 0.2220 - val_accuracy: 0.9167\n",
            "Epoch 21/30\n",
            "15/15 [==============================] - 35s 2s/step - loss: 0.0309 - accuracy: 0.9913 - val_loss: 0.2169 - val_accuracy: 0.9167\n",
            "Epoch 22/30\n",
            "15/15 [==============================] - 34s 2s/step - loss: 0.0185 - accuracy: 0.9956 - val_loss: 0.0784 - val_accuracy: 0.9583\n",
            "Epoch 23/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.0103 - accuracy: 0.9979 - val_loss: 0.1348 - val_accuracy: 0.9583\n",
            "Epoch 24/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.0215 - accuracy: 0.9913 - val_loss: 0.2698 - val_accuracy: 0.9271\n",
            "Epoch 25/30\n",
            "15/15 [==============================] - 30s 2s/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.1420 - val_accuracy: 0.9583\n",
            "Epoch 26/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.0261 - accuracy: 0.9869 - val_loss: 0.1930 - val_accuracy: 0.9479\n",
            "Epoch 27/30\n",
            "15/15 [==============================] - 29s 2s/step - loss: 0.0282 - accuracy: 0.9913 - val_loss: 0.1414 - val_accuracy: 0.9583\n",
            "Epoch 28/30\n",
            "15/15 [==============================] - 30s 2s/step - loss: 0.0300 - accuracy: 0.9847 - val_loss: 0.1763 - val_accuracy: 0.9479\n",
            "Epoch 29/30\n",
            "15/15 [==============================] - 30s 2s/step - loss: 0.0127 - accuracy: 0.9978 - val_loss: 0.1970 - val_accuracy: 0.9479\n",
            "Epoch 30/30\n",
            "15/15 [==============================] - 31s 2s/step - loss: 0.0081 - accuracy: 0.9978 - val_loss: 0.1543 - val_accuracy: 0.9688\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "train_data_dir = '/content/drive/MyDrive/trainnew'\n",
        "val_data_dir = '/content/drive/MyDrive/valnew'\n",
        "\n",
        "# Parameters\n",
        "num_classes = 2\n",
        "img_width, img_height = 224, 224\n",
        "batch_size = 32\n",
        "epochs = 30\n",
        "\n",
        "# Data augmentation and preprocessing\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "    val_data_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Load pre-trained MobileNetV2 model\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(img_width, img_height, 3))\n",
        "\n",
        "# Add custom classification layers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples ,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_generator.samples // batch_size\n",
        ")\n",
        "\n",
        "# Save the trained model\n",
        "model.save('poacher_detection_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('poacher_detection_model.h5')\n"
      ],
      "metadata": {
        "id": "Ebs_Z6PU-zf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = image.load_img(image_path, target_size=(224, 224))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = img / 255.0\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "d5F4y-p9-zjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_image_paths = [\n",
        "    '/content/drive/MyDrive/val/val1/img231.jpg',\n",
        "    '/content/drive/MyDrive/val/val1/img228.jpg',\n",
        "    '/content/drive/MyDrive/val/val1/i2.jpg',\n",
        "    '/content/drive/MyDrive/val/val1/img208.jpg',\n",
        "    '/content/drive/MyDrive/val/val1/img202.jpg',\n",
        "    '/content/drive/MyDrive/train/train1/056.jpg'\n",
        "\n",
        "]\n",
        "\n"
      ],
      "metadata": {
        "id": "QLxVTQRy_KEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_path in new_image_paths:\n",
        "    preprocessed_image = preprocess_image(image_path)\n",
        "    prediction = model.predict(preprocessed_image)[0]\n",
        "\n",
        "    if prediction > 0.5:\n",
        "        result = 'Poacher Detected'\n",
        "\n",
        "    else:\n",
        "        result = 'No Poacher Detected'\n",
        "    print(f\"Image: {image_path} - Prediction: {result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cc5xhOuz_KHk",
        "outputId": "bb824cf9-fd0e-47ef-d289-0abccadaf779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n",
            "Image: /content/drive/MyDrive/val/val1/img231.jpg - Prediction: No Poacher Detected\n",
            "1/1 [==============================] - 0s 112ms/step\n",
            "Image: /content/drive/MyDrive/val/val1/img228.jpg - Prediction: Poacher Detected\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "Image: /content/drive/MyDrive/val/val1/i2.jpg - Prediction: No Poacher Detected\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "Image: /content/drive/MyDrive/val/val1/img208.jpg - Prediction: Poacher Detected\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "Image: /content/drive/MyDrive/val/val1/img202.jpg - Prediction: Poacher Detected\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "Image: /content/drive/MyDrive/train/train1/056.jpg - Prediction: No Poacher Detected\n"
          ]
        }
      ]
    }
  ]
}